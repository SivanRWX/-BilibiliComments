import numpy as np
from PIL import Image
from pyecharts import options as opts
from pyecharts.charts import Pie
from datetime import datetime  # å¯¼å…¥æ—¥æœŸè½¬æ¢æ¨¡å—
import csv  # å¯¼å…¥csvæ¨¡å—
import hashlib  # å¯¼å…¥å“ˆå¸Œæ¨¡å—
from urllib.parse import quote  # å¯¼å…¥ç¼–ç çš„æ–¹æ³•
import time  # å¯¼å…¥æ—¶é—´æ¨¡å—
import pandas as pd  # å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
import jieba  # å¯¼å…¥åˆ†è¯æ¨¡å—
from wordcloud import WordCloud  # å¯¼å…¥è¯äº‘æ¨¡å—
from matplotlib import pyplot as plt
import random  # æ·»åŠ éšæœºå»¶è¿Ÿ
import requests

from DrissionPage import ChromiumPage

# Cookie å­—ç¬¦ä¸²
cookie_str = (
    "buvid3=B25FEA33-6B46-3C5E-A18A-C3E53763A9F932854infoc; "
    "b_nut=1747320632; "
    "_uuid=35A716DE-7109E-2293-9FF4-225FB6A5699234883infoc; "
    "CURRENT_FNVAL=4048; "
    "buvid4=F173D692-E496-759E-5D4E-88E0938645B834679-025051522-LAQ2E/2AzFkNNYOyQ++fIQ%3D%3D; "
    "rpdid=|(JY~|lk~)R)0J'u~R~Jk|YJR; "
    "SESSDATA=adedc65e%2C1762922652%2C49cf3%2A51CjDY4gBlocko40Qwo16KhMaFvccww4rFFKeNCURUEN9hEBlD2t_0Zj92oPECNZBWwF8SVnBhQ1NxQUFkUkk1clJ3WUp0QXR3MEJQSm8xdmJ3dlctUzJOQldXMmp3Qzl4UHJZN1JUcXdfUVRzZ2Jsb2wya2dzaU1xVEJlUGlxd2NLTV9qWVJ0V3ZnIIEC; "
    "bili_jct=8fe597bfefad11937b62ade21974effe; "
    "DedeUserID=473409553; "
    "DedeUserID__ckMd5=5a076712a01c326e; "
    "fingerprint=d4c24c7d411e000b2732c6d26a01f97a; "
    "buvid_fp_plain=undefined; "
    "buvid_fp=d4c24c7d411e000b2732c6d26a01f97a; "
    "bp_t_offset_473409553=1067817522205556736; "
    "b_lsid=D14B9A2B_196DD52284E; "
    "bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NDc3MzI2MjEsImlhdCI6MTc0NzQ3MzM2MSwicGx0IjotMX0.lQ1Vt_etkTigl3VKsYIyYvO55TsRNpp_H1NZbjSfJlk; "
    "bili_ticket_expires=1747732561; "
    "sid=6mbf7u9r"
)
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
output_csv = 'bilibili_comments.csv'
video_url = 'https://www.bilibili.com/video/BV1x54y1e7zf'


def get_sign(now_time, next_page):
    """è·å–ç­¾å"""
    pagination_str = '{"offset":%s}' % next_page

    """è·å–åŠ å¯†å‚æ•°w_rid"""
    params = [
        "mode=2",
        "oid=841786906",
        f"pagination_str={quote(pagination_str)}",
        "plat=1",
        "type=1",
        "web_location=1315875",
        f"wts={now_time}"
    ]
    string = '&'.join(params) + 'ea1db124af3c7062474693fa704f4ff8'
    w_rid = hashlib.md5(string.encode('utf-8')).hexdigest()
    print(w_rid)
    return w_rid, pagination_str


# åˆå§‹åŒ–æµè§ˆå™¨å¹¶è®¾ç½®cookie
def parse_cookies(cookie_str):
    """å°† Cookie å­—ç¬¦ä¸²è§£æä¸ºå­—å…¸åˆ—è¡¨"""
    cookies = []
    for item in cookie_str.split('; '):
        if '=' in item:
            name, value = item.split('=', 1)
            cookies.append({
                'name': name.strip(),
                'value': value.strip(),
                'domain': '.bilibili.com',
                'path': '/',
                # æœ‰æ•ˆæœŸè®¾ç½®ä¸º 1 å¤©ï¼ˆå•ä½ï¼šç§’ï¼‰
                'expires': int(time.time()) + 86400
            })
    return cookies


def crawl_comments():
    """ä¸»çˆ¬è™«é€»è¾‘"""

    # åˆ›å»ºæµè§ˆå™¨å¯¹è±¡å¹¶è®¾ç½® Cookie
    page = ChromiumPage()
    cookies = parse_cookies(cookie_str)
    page.set.cookies(cookies)

    # éªŒè¯ç™»å½•çŠ¶æ€
    page.get('https://www.bilibili.com')
    if page.title == 'å“”å“©å“”å“© (ã‚œ-ã‚œ)ã¤ãƒ­ å¹²æ¯~-bilibili':
        print('âœ… Cookie ç™»å½•æˆåŠŸ')
    else:
        print('âŒ ç™»å½•çŠ¶æ€å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ Cookie æœ‰æ•ˆæ€§')
        page.quit()
        exit()

    # è·å–æ—¶é—´æˆ³
    now_time = int(time.time())
    # è·å–åŠ å¯†å‚æ•°
    w_rid, pagination_str = get_sign(now_time, '{"offset":""}')

    # ç›‘å¬å¸¦ç­¾åçš„æ¥å£
    page.listen.start(f'api.bilibili.com/x/v2/reply/main?*w_rid={w_rid}*')

    # æŸ¥è¯¢å‚æ•° (åœ¨è½½è·ä¸­å¤åˆ¶) æŒ‰æœ€çƒ­æ’åˆ—
    params = {
        'oid': '841786906',
        'type': '1',
        'mode': '2',
        'pagination_str': pagination_str,
        'plat': '1',
        'web_location': '1315875',
        'w_rid': w_rid,
        'wts': now_time
    }

    # è®¿é—®è§†é¢‘é¡µé¢
    page.get(video_url)
    page.wait.load_start()

    with open(output_csv, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['æ˜µç§°', 'æ€§åˆ«', 'åœ°åŒº', 'æ—¥æœŸ', 'å†…å®¹', 'ç‚¹èµ'])
        writer.writeheader()

        collected = set()  # ç”¨äºå»é‡
        retry_count = 0

        # # è·³è½¬åˆ°æŠ“å–é¡µé¢
        # page.get(video_url)
        # page.wait(1)

        while len(collected) < 200 and retry_count < 5:  # æœ€å¤šé‡‡é›†200æ¡æˆ–é‡è¯•5æ¬¡
            # æ»šåŠ¨åˆ°è¯„è®ºåŒº
            page.scroll.to_bottom()

            # è·å–æ‰€æœ‰è¯„è®ºå¡ç‰‡
            comments = page.eles('.reply-item:not([data-loaded="true"])')

            for comment in comments:
                try:
                    # æ ‡è®°å·²å¤„ç†
                    comment.set.attr('data-loaded', 'true')

                    # æå–ç”¨æˆ·ä¿¡æ¯
                    user_info = comment.ele('.user-info')
                    username = user_info.ele('.user-name').text
                    gender = 'ç”·' if 'male' in user_info.ele('.user-gender').attr('class') else 'å¥³'

                    # æå–IPå±åœ°
                    location = comment.ele('.ip-location').text.replace('IPå±åœ°ï¼š', '')

                    # æå–è¯„è®ºå†…å®¹
                    content = comment.ele('.text-con').text

                    # æå–ç‚¹èµæ•°
                    like = comment.ele('.like span').text

                    # æ—¶é—´å¤„ç†
                    time_str = comment.ele('.time').text
                    comment_time = datetime.now().strftime('%Y-%m-%d')  # å¦‚æœæ—¶é—´æ˜¯ç›¸å¯¹æ—¶é—´éœ€è¦é¢å¤–å¤„ç†

                    # å»é‡æ£€æŸ¥
                    if content not in collected:
                        writer.writerow({
                            'æ˜µç§°': username,
                            'æ€§åˆ«': gender,
                            'åœ°åŒº': location,
                            'æ—¥æœŸ': comment_time,
                            'å†…å®¹': content,
                            'ç‚¹èµ': like
                        })
                        collected.add(content)
                        print(f'âœ… å·²é‡‡é›†ï¼š{username} çš„è¯„è®º')
                except Exception as e:
                    print(f'âš ï¸ è§£æå¤±è´¥ï¼š{str(e)}')
                    continue

            # æ»šåŠ¨åŠ è½½æ›´å¤š
            page.scroll.down(800)
            time.sleep(random.uniform(1.5, 3.5))

            # æ£€æŸ¥æ˜¯å¦åˆ°åº•
            if page.ele('.no-more', timeout=2):
                print('å·²åŠ è½½å…¨éƒ¨è¯„è®º')
                break

            # é‡è¯•æœºåˆ¶
            if not comments:
                retry_count += 1
                page.scroll.down(1000)
                time.sleep(2)
            else:
                retry_count = 0

        print(f'ğŸ‰ é‡‡é›†å®Œæˆï¼Œå…±è·å¾— {len(collected)} æ¡è¯„è®º')
        page.quit()


def process_comments(data, writer):
    """å¤„ç†è¯„è®ºæ•°æ®"""
    for reply in data['data']['replies']:
        try:
            ctime = reply['ctime']
            writer.writerow({
                'æ˜µç§°': reply['member']['uname'],
                'æ€§åˆ«': reply['member']['sex'],
                'åœ°åŒº': reply['reply_control']['location'].replace('IPå±åœ°ï¼š', ''),
                'æ—¥æœŸ': datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S'),
                'å†…å®¹': reply['content']['message'],
                'ç‚¹èµ': reply['like']
            })
            print(f"å·²é‡‡é›†ï¼š{reply['member']['uname']}çš„è¯„è®º")
        except Exception as e:
            print(f"è§£æå¤±è´¥ï¼š{str(e)}")


def generate_visualization():
    # åœ°åŒºåˆ†å¸ƒå¯è§†åŒ–
    def create_area_chart():
        df = pd.read_csv('bilibili_comments.csv')
        area_counts = df['åœ°åŒº'].value_counts()

        pie = (
            Pie()
            .add("", [list(z) for z in zip(area_counts.index.tolist(), area_counts.values.tolist())])
            .set_global_opts(
                title_opts=opts.TitleOpts(title="Bç«™è¯„è®ºåœ°åŒºåˆ†å¸ƒ"),
                legend_opts=opts.LegendOpts(pos_left="right", orient="vertical")
            )
            .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c}"))
        )
        pie.render("bilibili_area_distribution.html")

    # ç”Ÿæˆè¯äº‘
    def create_wordcloud():
        df = pd.read_csv('bilibili_comments.csv')
        content = ' '.join(df['å†…å®¹'].astype(str))

        # ä½¿ç”¨è‡ªå®šä¹‰å½¢çŠ¶
        mask = np.array(Image.open('mask_shape.png'))  # å‡†å¤‡ä¸€ä¸ªPNGå½¢çŠ¶æ–‡ä»¶

        wc = WordCloud(
            font_path='msyh.ttc',
            background_color='white',
            max_words=200,
            mask=mask,
            stopwords={'äº†', 'çš„', 'æ˜¯', 'æˆ‘', 'åœ¨', 'æœ‰', 'å°±', 'ä¹Ÿ', 'å•Š', 'è¿™ä¸ª'}
        )

        words = ' '.join(jieba.lcut(content))
        wc.generate(words)

        plt.figure(figsize=(10, 8))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis("off")
        plt.savefig('bilibili_wordcloud.png')
        plt.show()


if __name__ == '__main__':
    crawl_comments()
    generate_visualization()
